{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# # uncomment the following lines to install the required packages\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "# !{sys.executable} -m pip install gmsh\n",
    "# !{sys.executable} -m pip install scipy\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install Pyarrow\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# !{sys.executable} -m pip install pyvista\n",
    "# !{sys.executable} -m pip install ipywidgets\n",
    "# !{sys.executable} -m pip install pyvirtualdisplay\n",
    "# !{sys.executable} -m pip install plotly\n",
    "# !{sys.executable} -m pip install -U kaleido\n",
    "# !{sys.executable} -m pip install pdf2image\n",
    "# !{sys.executable} -m pip install imageio\n",
    "# !{sys.executable} -m pip install imageio[ffmpeg]\n",
    "# !{sys.executable} -m pip install PyPDF2\n",
    "# !{sys.executable} -m pip install PyMuPDF\n",
    "# !{sys.executable} -m pip install IPython\n",
    "\n",
    "# print(\"All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels and colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature\n",
    "color_temperature = 'coolwarm'\n",
    "label_temperature = r'Temperature $T$ [$^\\circ$C]'\n",
    "\n",
    "# gradient\n",
    "color_gradient = 'Spectral'\n",
    "label_gradient_partial = r'Gradient $\\nabla T$ [$^\\circ$C/m]'\n",
    "label_gradient_partial_x = r'Gradient $\\partial T / \\partial x$ [$^\\circ$C/m]'\n",
    "label_gradient_partial_y = r'Gradient $\\partial T / \\partial y$ [$^\\circ$C/m]'\n",
    "label_gradient_g = r'Gradient $\\mathbf g$ [$^\\circ$C/m]'\n",
    "label_gradient_g_x = r'Gradient $g_x$ [$^\\circ$C/m]'\n",
    "label_gradient_g_y = r'Gradient $g_y$ [$^\\circ$C/m]'\n",
    "\n",
    "# flux\n",
    "color_flux = 'seismic'\n",
    "label_flux = r'Flux $\\mathbf q$ [W/m$^2$]'\n",
    "label_flux_x = r'Flux $q_x$ [W/m$^2$]'\n",
    "label_flux_y = r'Flux $q_y$ [W/m$^2$]'\n",
    "\n",
    "# conductivity\n",
    "color_conductivity = 'viridis'\n",
    "label_conductivity = r'Thermal conductivity $k$ [W/(m$\\cdot$$^\\circ$C)]'\n",
    "\n",
    "# source\n",
    "color_source_min = 'black'\n",
    "color_source_max = 'red'\n",
    "label_source = r'Source term $s$ [W/m$^3$]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# Create a custom colormap (blue to red without white)\n",
    "colors = [(0, 0, 1), (1, 0, 0)]  # Blue to Red\n",
    "n_bins = 100\n",
    "cmap_name = 'blue_to_red'\n",
    "custom_cmap_temperature = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\n",
    "colors = ['green', 'yellow', 'magenta']\n",
    "cmap_name = 'green_to_yellow'\n",
    "custom_cmap_flux = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\n",
    "colors = ['orange', 'yellow', 'purple']\n",
    "cmap_name = 'orange_to_purple'\n",
    "custom_cmap_grad = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\n",
    "custom_colorscale_temperature = [\n",
    "    [0, 'blue'],\n",
    "    [1, 'red']\n",
    "]\n",
    "\n",
    "custom_colorscale_flux = [\n",
    "    [0, 'green'],\n",
    "    [0.5, 'yellow'],\n",
    "    [1, 'magenta']\n",
    "]\n",
    "\n",
    "# custom_colorscale_flux = [\n",
    "#     [0, '#0000FF'],  # Blue\n",
    "#     [0.5, '#FFFFFF'],  # White\n",
    "#     [1, '#FF0000']  # Red\n",
    "# ]\n",
    "\n",
    "custom_colorscale_grad = [\n",
    "    [0, 'orange'],\n",
    "    [0.5, 'yellow'],\n",
    "    [1, 'purple']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting graphs settings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.transforms import Bbox\n",
    "import seaborn as sns\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [9, 6]\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "# plt.rcParams['font.family'] = \"Serif\"\n",
    "plt.rcParams['font.family'] = \"Times New Roman\"\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# colour pallete\n",
    "# plt.rcParams['axes.prop_cycle'] = plt.cycler(\"color\", plt.cm.tab20.colors)\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Apply the default settings to Seaborn\n",
    "sns.set_context(rc=plt.rcParams)\n",
    "\n",
    "# create mesh with gmsh\n",
    "import gmsh\n",
    "\n",
    "# plotting vtks\n",
    "import pyvista as pv\n",
    "pv.set_plot_theme(\"document\")\n",
    "import ipywidgets as widgets\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "print(\"Imports finished! :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations of the executables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directories\n",
    "um_view = \"$HOME/um_view_release\" # change this to the location of your um_view_release or um_view with mofem_data_driven_finite_elements installed\n",
    "tools_dir = um_view + \"/bin\"\n",
    "dd_dir = um_view + \"/mofem_data_driven_finite_elements\"\n",
    "\n",
    "# tools\n",
    "read_med = tools_dir + \"/read_med\"\n",
    "mofem_part = tools_dir + \"/mofem_part\"\n",
    "\n",
    "# create datasets\n",
    "create_csv_dataset = dd_dir + \"/create_csv_dataset\"\n",
    "\n",
    "# classic diffusion\n",
    "classic_diffusion = dd_dir + \"/classic_diffusion\"\n",
    "hdiv_diffusion = dd_dir + \"/hdiv_diffusion\"\n",
    "\n",
    "# data driven diffusion\n",
    "data_driven_diffusion = dd_dir + \"/data_driven_diffusion\"\n",
    "data_driven_diffusion_snes = dd_dir + \"/data_driven_diffusion_snes\"\n",
    "hdiv_data_driven_diffusion = dd_dir + \"/hdiv_data_driven_diffusion\"\n",
    "hdiv_data_driven_diffusion_snes = dd_dir + \"/hdiv_data_driven_diffusion_snes\"\n",
    "\n",
    "# nonlinear diffusion\n",
    "diffusion_nonlinear_graphite = dd_dir + \"/diffusion_nonlinear_graphite\"\n",
    "diffusion_nonlinear_hdiv_graphite = dd_dir + \"/diffusion_nonlinear_hdiv_graphite\"\n",
    "diffusion_nonlinear_snes = dd_dir + \"/diffusion_nonlinear_snes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self:\n",
    "            return self[attr]\n",
    "        raise AttributeError(f\"'AttrDict' object has no attribute '{attr}'\")\n",
    "    \n",
    "params = AttrDict() # Attribute dictionary for storing the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colobars ticks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_colorbar_ticks(cbar, data, decimal_places, add_min_max=True):\n",
    "    # Get default ticks\n",
    "    ticks = cbar.get_ticks()\n",
    "\n",
    "    # Add min and max to default ticks if specified\n",
    "    if add_min_max:\n",
    "        ticks = np.unique(np.append([data.min(), data.max()], ticks))\n",
    "\n",
    "    # Ensure ticks are within range\n",
    "    ticks = ticks[(ticks >= data.min()) & (ticks <= data.max())]\n",
    "\n",
    "    cbar.set_ticks(ticks)\n",
    "    cbar.formatter = FuncFormatter(lambda x, pos: f'{x:.{decimal_places}f}'.rstrip('0').rstrip('.'))\n",
    "    cbar.update_ticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraview plots showing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(params):\n",
    "    out_to_vtk = !ls -c1 {params.show_file}*vtk\n",
    "    last_file=out_to_vtk[0]\n",
    "\n",
    "    p = pv.Plotter(notebook=True)\n",
    "\n",
    "    mesh = pv.read(last_file[:-3] + \"vtk\")\n",
    "    warped_mesh = mesh\n",
    "\n",
    "    if params.warp_field_scalar:\n",
    "        # p.add_mesh(mesh, show_scalar_bar = False, opacity=0.2, color='gray', smooth_shading=True,)\n",
    "        warped_mesh = mesh.warp_by_scalar(scalars=params.warp_field_scalar, factor=params.warp_factor)\n",
    "    \n",
    "    if params.warp_field_vector:\n",
    "        # p.add_mesh(mesh, show_scalar_bar = False, opacity=0.2, color='gray', smooth_shading=True,)\n",
    "        warped_mesh = mesh.warp_by_vector(vectors=params.warp_field_vector, factor=params.warp_factor)\n",
    "\n",
    "    if params.show_edges:\n",
    "        warped_mesh=warped_mesh.shrink(0.95)\n",
    "    \n",
    "    jupyter_backend='ipygany'\n",
    "\n",
    "    # Check if the field is in point data or cell data\n",
    "    if params.show_field in warped_mesh.point_data:\n",
    "        data_location = warped_mesh.point_data\n",
    "    elif params.show_field in warped_mesh.cell_data:\n",
    "        data_location = warped_mesh.cell_data\n",
    "    else:\n",
    "        raise KeyError(f\"Field '{params.show_field}' not found in point or cell data.\")\n",
    "\n",
    "    # TODO: Add support for showing the field part for the gradient\n",
    "    # Get the scalars for coloring the points\n",
    "    if (params.field_part < 0):\n",
    "        scalars = data_location[params.show_field]\n",
    "    elif params.field_part == 10: # plot gradient\n",
    "        warped_mesh = warped_mesh.compute_derivative(scalars=params.show_field, preference='point')\n",
    "        scalars = warped_mesh.point_data['gradient']\n",
    "    else:\n",
    "        scalars = warped_mesh.point_data[params.show_field][:,params.field_part]\n",
    "\n",
    "    scalars = scalars * params.show_field_scale \n",
    "\n",
    "    if params.show_ori_shape:\n",
    "        p.add_mesh(mesh, component=None, smooth_shading=True, opacity=0.5, color='gray')\n",
    "\n",
    "\n",
    "    scalar_bar_title = params.show_field_name if params.show_field_name else params.show_field\n",
    "    # Define the arguments for the scalar bar\n",
    "    scalar_bar_args = {\n",
    "        # 'n_labels': 8,\n",
    "        'position_x': 0.2,  \n",
    "        'title': scalar_bar_title,\n",
    "    }\n",
    "\n",
    "    p.add_mesh(warped_mesh, scalars=scalars, component=None, smooth_shading=False, cmap=params.p_cmap, clim = params.clim, show_scalar_bar=True, opacity=0.9, scalar_bar_args=scalar_bar_args)\n",
    "\n",
    "    # if params.show_scalar_bar:\n",
    "    #     scalar_bar_title = params.show_field if params.show_field else \"Scalar Field\"\n",
    "    #     p.add_scalar_bar(title=scalar_bar_title, color='black')\n",
    " \n",
    "    p.camera_position = params.camera_position\n",
    "    \n",
    "    p.enable_parallel_projection()\n",
    "    p.enable_image_style()\n",
    "    \n",
    "    p.show(jupyter_backend=jupyter_backend)\n",
    "    if params.p_save:\n",
    "        p.save_graphic(params.p_save) \n",
    "\n",
    "def show_resulting_points(params):\n",
    "    out_to_vtk = !ls -c1 {params.show_file}*vtk\n",
    "    last_file=out_to_vtk[0]\n",
    "\n",
    "    p = pv.Plotter(notebook=True)\n",
    "\n",
    "    mesh = pv.read(last_file[:-3] + \"vtk\")\n",
    "\n",
    "    # check if the fields are in point data or cell data or exist at all\n",
    "    if params.show_field not in mesh.point_data:\n",
    "        # stop function if the field does not exist\n",
    "        # do not raise error just a warning and stop\n",
    "        print(f\"Field '{params.show_field}' not found in point data.\")\n",
    "        return\n",
    "    \n",
    "    if params.show_field_2 and params.show_field_2 not in mesh.point_data:\n",
    "        # stop function if the field does not exist\n",
    "        # do not raise error just a warning and stop\n",
    "        print(f\"Field '{params.show_field_2}' not found in point data.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Create a point cloud from the mesh points\n",
    "    point_cloud = pv.PolyData(mesh.points)\n",
    "\n",
    "    # Check if params.show_field_2 is defined and calculate the difference if so\n",
    "    if (params.show_field_2):\n",
    "        if (params.field_part < 0):\n",
    "            scalars1 = mesh.point_data[params.show_field]\n",
    "            scalars2 = mesh.point_data[params.show_field_2]\n",
    "        else:\n",
    "            scalars1 = mesh.point_data[params.show_field][:, params.field_part]\n",
    "            scalars2 = mesh.point_data[params.show_field_2][:, params.field_part]\n",
    "        \n",
    "        scalars = scalars1 * params.show_field_scale - scalars2 * params.show_field_scale_2\n",
    "        if params.show_field_diff_abs:\n",
    "            scalars = np.abs(scalars)\n",
    "    else:\n",
    "        # Get the scalars for coloring the points\n",
    "        if (params.field_part < 0):\n",
    "            scalars = mesh.point_data[params.show_field]\n",
    "        else:\n",
    "            scalars = mesh.point_data[params.show_field][:, params.field_part]\n",
    "\n",
    "    # Define the arguments for the scalar bar\n",
    "    scalar_bar_title = params.show_field_name if params.show_field_name else params.show_field\n",
    "    scalar_bar_args = {\n",
    "        # 'n_labels': 8,\n",
    "        'position_x': 0.2,  \n",
    "        'title': scalar_bar_title,\n",
    "    }\n",
    "\n",
    "    # Add the point cloud to the plot with coloring based on the scalars\n",
    "    p.add_points(point_cloud, scalars=scalars, cmap=params.p_cmap, clim=params.clim, point_size=5, scalar_bar_args=scalar_bar_args)\n",
    "\n",
    "    p.camera_position = params.camera_position\n",
    "    \n",
    "    p.enable_parallel_projection()\n",
    "    p.enable_image_style()\n",
    "    \n",
    "    p.show(jupyter_backend='ipygany')\n",
    "    if params.p_save:\n",
    "        p.save_graphic(params.p_save)\n",
    "\n",
    "params.warp_field_scalar = \"\"    # no warp field\n",
    "params.warp_field_vector = \"\"    # no warp field\n",
    "params.warp_factor = 1.0  # warp factor\n",
    "params.show_edges = True # show edges\n",
    "params.triangle_mesh = True # triangle mesh\n",
    "params.config_file = \"bc.cfg\" # config file\n",
    "params.conductivity = 1.0 # linear conductivity\n",
    "params.camera_position = \"xy\" # camera position\n",
    "params.clim = None # colorbar limits\n",
    "params.p_save = None\n",
    "params.p_cmap = \"turbo\"\n",
    "params.show_scalar_bar = True\n",
    "params.show_field = None\n",
    "params.field_part = -1\n",
    "params.show_ori_shape = False\n",
    "params.show_field_name = None\n",
    "\n",
    "# for show_resulting_points with 2 fields\n",
    "params.show_field_2 = None\n",
    "params.show_field_scale = 1.0\n",
    "params.show_field_scale_2 = 1.0\n",
    "params.show_field_diff_abs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self, ana_name, naming, order_list, error_name_list, error_label_list, filename_prefix=\"\", elem_size=None, marker='o', linestyle='-', plot_gradients=True, label=None, color='b', legend_fond_size = None):\n",
    "        self.ana_name = ana_name\n",
    "        self.naming = naming\n",
    "        self.order_list = order_list\n",
    "        self.error_name_list = error_name_list\n",
    "        self.error_label_list = error_label_list\n",
    "        self.filename_prefix = filename_prefix\n",
    "        self.elem_size = elem_size\n",
    "        self.plot_gradients = plot_gradients\n",
    "        self.color = color\n",
    "        self.marker = marker\n",
    "        self.linestyle = linestyle\n",
    "        self.legend_fond_size = legend_fond_size\n",
    "        self.label = \"; \"+label if label else \"\"\n",
    "        self.data = self._read_analysis_results()\n",
    "        self._add_total_error()\n",
    "        self.data_by_order = self._sort_data_by_order()\n",
    "        self.gradient_sf = 1\n",
    "\n",
    "    def _read_analysis_results(self):\n",
    "        return pd.read_csv(f'{self.ana_name}.csv', header=None, names=self.naming,  index_col=False)\n",
    "    \n",
    "    def _add_total_error(self):\n",
    "        # if columns L2norm H1seminorm fluxErr exist in the data add them together to get the total error\n",
    "        if 'L2norm' in self.data.columns and 'H1seminorm' in self.data.columns and 'fluxErr' in self.data.columns:\n",
    "            self.data['totalErr'] = self.data['L2norm'] + self.data['H1seminorm'] + self.data['fluxErr']\n",
    "\n",
    "    def _sort_data_by_order(self):\n",
    "        return [self.data.query(f'order == {i}') for i in self.order_list]\n",
    "\n",
    "    def _plot_error(self, plot_order_func, filename_prefix, xlabel, x):\n",
    "        for (error_name, error_label) in zip(self.error_name_list, self.error_label_list):\n",
    "            fig, ax = plt.subplots()\n",
    "            for i, order in enumerate(self.order_list):\n",
    "                plot_order_func(ax, i, order, error_name, error_label, xlabel, x)\n",
    "            self._save_plot(f'{filename_prefix}{error_name}')\n",
    "\n",
    "    def _plot_order_by_variable(self, ax, i, order, error_name, error_label, xlabel, x, color='b', linestyle='-', marker='o', line_label=\"\"):\n",
    "        y = self.data_by_order[i][str(error_name)]\n",
    "        self._plot_and_annotate(ax, x, y, order, error_name, error_label, xlabel, color, linestyle, marker, line_label)\n",
    "\n",
    "    def _plot_and_annotate(self, ax, x, y, order, error_name, error_label, xlabel, color='b', linestyle='-', marker='o', line_label=\"\"):\n",
    "        if type(x) != list:\n",
    "            x = x.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        line, = ax.plot(x, y, linestyle+marker, label=f'order = {order}{line_label}', color=color)\n",
    "        if self.plot_gradients:\n",
    "            gradient, _ = np.polyfit(np.log(x), np.log(y), 1)\n",
    "            ax.annotate(f'Gradient = {gradient:.{self.gradient_sf}f}', \n",
    "                        xy=(x[len(x)//2], y[len(y)//2]),\n",
    "                        xycoords='data',\n",
    "                        xytext=(-1, 0),\n",
    "                        textcoords='offset points',\n",
    "                        ha='right',\n",
    "                        va='bottom',\n",
    "                        )\n",
    "        ax.set_xscale('log')\n",
    "        ax.legend(loc='best')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylabel(error_label)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.grid(True, ls=':')\n",
    "\n",
    "    def _save_plot(self, filename):\n",
    "        filename = f'{self.filename_prefix}{filename}'\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{filename}.svg')\n",
    "        plt.savefig(f'{filename}.png')\n",
    "        plt.savefig(f'{filename}.pdf')\n",
    "\n",
    "    def plot_both_analyses_by_elem_size(self, extraAnas=[]):\n",
    "        if self.elem_size is None:\n",
    "            raise ValueError(\"elem_size is not provided for one or both analyses\")\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "        for (error_name, error_label) in zip(self.error_name_list, self.error_label_list):\n",
    "            fig, ax = plt.subplots()\n",
    "            self_label = self.label if extraAnas else \"\"\n",
    "            filename_part = \"compare_\" if extraAnas else \"\"\n",
    "            for i, (order_self) in enumerate(zip(self.data_by_order)):\n",
    "                color = colors[i % len(colors)]\n",
    "                self._plot_order_by_variable(ax, i, self.order_list[i], error_name, error_label, 'Element size', self.elem_size, color=color, linestyle=self.linestyle, marker=self.marker, line_label=self_label)\n",
    "                for extra_ana in extraAnas:\n",
    "                    extra_ana._plot_order_by_variable(ax, i, extra_ana.order_list[i], error_name, error_label, 'Element size', self.elem_size, color=color, linestyle=extra_ana.linestyle, marker=extra_ana.marker, line_label=extra_ana.label)\n",
    "            \n",
    "            if self.legend_fond_size:\n",
    "                ax.legend(fontsize=self.legend_fond_size,loc='best')\n",
    "            self._save_plot(f'elem_size_{filename_part}{error_name}')\n",
    "\n",
    "    def plot_both_analyses_by_gaussnum(self, extraAnas=[], extraAnas_singles=[], monte_data_list=[]):\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "        for (error_name, error_label) in zip(self.error_name_list, self.error_label_list):\n",
    "            fig, ax = plt.subplots()\n",
    "            self_label = self.label if extraAnas else \"\"\n",
    "            filename_part = \"compare_\" if extraAnas else \"\"\n",
    "            for i, (order_self) in enumerate(zip(self.data_by_order)):\n",
    "                color = colors[i % len(colors)]\n",
    "                self._plot_order_by_variable(ax, i, self.order_list[i], error_name, error_label, 'Gauss number', self.data_by_order[i][\"gaussnum\"], color=color, linestyle=self.linestyle, marker=self.marker, line_label=self_label)\n",
    "                for extra_ana in extraAnas:\n",
    "                    extra_ana._plot_order_by_variable(ax, i, extra_ana.order_list[i], error_name, error_label, 'Gauss number', extra_ana.data_by_order[i][\"gaussnum\"], color=color, linestyle=extra_ana.linestyle, marker=extra_ana.marker, line_label=extra_ana.label)\n",
    "            for extra_ana in extraAnas_singles:\n",
    "                extra_ana._plot_and_annotate(ax, extra_ana.data[\"gaussnum\"], extra_ana.data[error_name], extra_ana.data[\"order\"][0], error_name, error_label, 'Gauss number', color=extra_ana.color, linestyle=extra_ana.linestyle, marker=extra_ana.marker, line_label=extra_ana.label)\n",
    "            for monte_data in monte_data_list:         \n",
    "                plt.scatter(monte_data[\"gaussnum\"], monte_data[error_name], marker='x', label='Monte Carlo')\n",
    "            if self.legend_fond_size:\n",
    "                ax.legend(fontsize=self.legend_fond_size,loc='best')\n",
    "            self._save_plot(f'gaussnum_{filename_part}{error_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read_med and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMed(params):\n",
    "\n",
    "    # translate .med file to a format readable by MoFEM and assign values to physical groups\n",
    "    h5m_file=params.mesh_file + \".h5m\"    \n",
    "    !{read_med} -med_file {params.med_file} -output_file {h5m_file} -meshsets_config {params.config_file} -dim 2 -adj_dim 1 -log_sl error\n",
    "    \n",
    "    # visualise the mesh\n",
    "    if params.show_mesh:\n",
    "        vtk_file=params.mesh_file + \".vtk\"\n",
    "        !mbconvert {h5m_file} {vtk_file}\n",
    "\n",
    "        mesh = pv.read(vtk_file)\n",
    "        mesh = mesh.shrink(0.98)\n",
    "\n",
    "        p = pv.Plotter(notebook=True)\n",
    "        p.add_mesh(mesh, smooth_shading=False)\n",
    "\n",
    "        p.camera_position = \"xy\"\n",
    "        p.show(jupyter_backend='ipygany')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square_top analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of a config file - what attributes should the blocksets have\n",
    "def generateConfig_squareTop(params):\n",
    "    # Open the file for writing\n",
    "    with open(params.config_file, 'w') as f:\n",
    "        # FLUX_SQUARE_TOP boundary condition\n",
    "        data = ['[SET_ATTR_FLUX_SQUARE_TOP]', 'number_of_attributes=1', 'user1='+str(params.conductivity), ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # PRESSURE_UNIFORM boundary condition set to 0\n",
    "        data = ['[SET_ATTR_PRESSURE_UNIFORM_0]', 'number_of_attributes=1', 'user1=0.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmsh creation of a 3D beam + visualisation of it\n",
    "def generateMesh_squareTop(params):\n",
    "    # Initialize gmsh\n",
    "    gmsh.initialize()\n",
    "    gmsh.option.setNumber(\"General.Verbosity\", 3)\n",
    "\n",
    "    square1 = gmsh.model.occ.add_rectangle(0, 0, 0, params.length_x, params.length_y)\n",
    "\n",
    "    # Create the relevant Gmsh data structures from Gmsh model.\n",
    "    gmsh.model.occ.synchronize()\n",
    "\n",
    "    # # ensuring a structured mesh with required element size \n",
    "    N = int(params.length_x / params.element_size) + 1\n",
    "\n",
    "    for n in range(len(gmsh.model.getEntities(1))):\n",
    "        gmsh.model.mesh.setTransfiniteCurve(n+1, N,'Progression', 1.0)\n",
    "\n",
    "    gmsh.model.mesh.setTransfiniteSurface(square1)\n",
    "\n",
    "    # gmsh.model.addPhysicalGroup(dimention, [number of element], name=\"name\")\n",
    "    gmsh.model.addPhysicalGroup(1, [3], name=\"FLUX_SQUARE_TOP\")\n",
    "    gmsh.model.addPhysicalGroup(1, [1,2,4], name=\"PRESSURE_UNIFORM_0\")\n",
    "    gmsh.model.addPhysicalGroup(2, [square1], name=\"DOMAIN\")\n",
    "\n",
    "    # for possible square meshes\n",
    "    if not params.triangle_mesh:\n",
    "        gmsh.option.setNumber(\"Mesh.RecombineAll\", 1)\n",
    "\n",
    "    # generate a 3D mesh\n",
    "    gmsh.model.mesh.generate(2)\n",
    "    \n",
    "    # save as a .med file\n",
    "    med_file = params.mesh_file + \".med\"\n",
    "    gmsh.write(med_file)\n",
    "    \n",
    "    # close gmsh\n",
    "    gmsh.finalize()\n",
    "    \n",
    "    # translate .med file to a format readable by MoFEM and assign values to physical groups\n",
    "    h5m_file=params.mesh_file + \".h5m\"    \n",
    "    !{read_med} -med_file {med_file} -output_file {h5m_file} -meshsets_config {params.config_file} -dim 2 -adj_dim 1 -log_sl error\n",
    "    \n",
    "    # visualise the mesh\n",
    "    if params.show_mesh:\n",
    "        vtk_file=params.mesh_file + \".vtk\"\n",
    "        !mbconvert {h5m_file} {vtk_file}\n",
    "\n",
    "        mesh = pv.read(vtk_file)\n",
    "        mesh = mesh.shrink(0.98)\n",
    "\n",
    "        p = pv.Plotter(notebook=True)\n",
    "        p.add_mesh(mesh, smooth_shading=False)\n",
    "\n",
    "        p.camera_position = \"xy\"\n",
    "        p.show(jupyter_backend='ipygany')\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square Sin Cos analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of a config file - what attributes should the blocksets have\n",
    "def generateConfig_squareSinCos(params):\n",
    "    # Open the file for writing\n",
    "    with open(params.config_file, 'w') as f:\n",
    "        # FLUX_SQUARE_SINCOS boundary condition\n",
    "        data = ['[SET_ATTR_FLUX_SQUARE_SINCOS]', 'number_of_attributes=1', 'user1='+str(params.conductivity), ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # PRESSURE_SQUARE_SINCOS boundary condition\n",
    "        data = ['[SET_ATTR_PRESSURE_SQUARE_SINCOS]', 'number_of_attributes=1', 'user1=1.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # SOURCE_SQUARE_SINCOS\n",
    "        data = ['[SET_ATTR_SOURCE_SQUARE_SINCOS]', 'number_of_attributes=1', 'user1=1.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmsh creation of a 3D beam + visualisation of it\n",
    "def generateMesh_squareSinCos(params):\n",
    "    # Initialize gmsh\n",
    "    gmsh.initialize()\n",
    "    gmsh.option.setNumber(\"General.Verbosity\", 3)\n",
    "\n",
    "    square1 = gmsh.model.occ.add_rectangle(0, 0, 0, params.length_x, params.length_y)\n",
    "\n",
    "    # Create the relevant Gmsh data structures from Gmsh model.\n",
    "    gmsh.model.occ.synchronize()\n",
    "\n",
    "    # # ensuring a structured mesh with required element size \n",
    "    N = int(params.length_x / params.element_size) + 1\n",
    "\n",
    "    for n in range(len(gmsh.model.getEntities(1))):\n",
    "        gmsh.model.mesh.setTransfiniteCurve(n+1, N,'Progression', 1.0)\n",
    "\n",
    "    gmsh.model.mesh.setTransfiniteSurface(square1)\n",
    "\n",
    "    # gmsh.model.addPhysicalGroup(dimention, [number of element], name=\"name\")\n",
    "    gmsh.model.addPhysicalGroup(1, [3], name=\"FLUX_SQUARE_SINCOS\")\n",
    "    gmsh.model.addPhysicalGroup(1, [1,2,4], name=\"PRESSURE_SQUARE_SINCOS\")\n",
    "    gmsh.model.addPhysicalGroup(2, [square1], name=\"SOURCE_SQUARE_SINCOS\")\n",
    "\n",
    "    # for possible square meshes\n",
    "    if not params.triangle_mesh:\n",
    "        gmsh.option.setNumber(\"Mesh.RecombineAll\", 1)\n",
    "\n",
    "    # generate a 3D mesh\n",
    "    gmsh.model.mesh.generate(2)\n",
    "    \n",
    "    # save as a .med file\n",
    "    med_file = params.mesh_file + \".med\"\n",
    "    gmsh.write(med_file)\n",
    "    \n",
    "    # close gmsh\n",
    "    gmsh.finalize()\n",
    "    \n",
    "    # translate .med file to a format readable by MoFEM and assign values to physical groups\n",
    "    h5m_file=params.mesh_file + \".h5m\"    \n",
    "    !{read_med} -med_file {med_file} -output_file {h5m_file} -meshsets_config {params.config_file} -dim 2 -adj_dim 1 -log_sl error\n",
    "    \n",
    "    # visualise the mesh\n",
    "    if params.show_mesh:\n",
    "        vtk_file=params.mesh_file + \".vtk\"\n",
    "        !mbconvert {h5m_file} {vtk_file}\n",
    "\n",
    "        mesh = pv.read(vtk_file)\n",
    "        mesh = mesh.shrink(0.98)\n",
    "\n",
    "        p = pv.Plotter(notebook=True)\n",
    "        p.add_mesh(mesh, smooth_shading=False)\n",
    "\n",
    "        p.camera_position = \"xy\"\n",
    "        p.show(jupyter_backend='ipygany')\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square NONLINEAR Sin Cos analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of a config file - what attributes should the blocksets have\n",
    "def generateConfig_squareNonlinearSinCos(params):\n",
    "    # Open the file for writing\n",
    "    with open(params.config_file, 'w') as f:\n",
    "        # FLUX_SQUARE_SINCOS boundary condition\n",
    "        data = ['[SET_ATTR_FLUX_SQUARE_NONLINEAR_SINCOS]', 'number_of_attributes=1', 'user1='+str(params.conductivity), ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # PRESSURE_SQUARE_SINCOS boundary condition\n",
    "        data = ['[SET_ATTR_PRESSURE_SQUARE_SINCOS]', 'number_of_attributes=1', 'user1=1.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # SOURCE_SQUARE_SINCOS\n",
    "        data = ['[SET_ATTR_SOURCE_SQUARE_NONLINEAR_SINCOS]', 'number_of_attributes=1', 'user1=1.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmsh creation of a 3D beam + visualisation of it\n",
    "def generateMesh_squareNonlinearSinCos(params):\n",
    "    # Initialize gmsh\n",
    "    gmsh.initialize()\n",
    "    gmsh.option.setNumber(\"General.Verbosity\", 3)\n",
    "\n",
    "    square1 = gmsh.model.occ.add_rectangle(0, 0, 0, params.length_x, params.length_y)\n",
    "\n",
    "    # Create the relevant Gmsh data structures from Gmsh model.\n",
    "    gmsh.model.occ.synchronize()\n",
    "\n",
    "    # # ensuring a structured mesh with required element size \n",
    "    N = int(params.length_x / params.element_size) + 1\n",
    "\n",
    "    for n in range(len(gmsh.model.getEntities(1))):\n",
    "        gmsh.model.mesh.setTransfiniteCurve(n+1, N,'Progression', 1.0)\n",
    "\n",
    "    gmsh.model.mesh.setTransfiniteSurface(square1)\n",
    "\n",
    "    # gmsh.model.addPhysicalGroup(dimention, [number of element], name=\"name\")\n",
    "    gmsh.model.addPhysicalGroup(1, [3], name=\"FLUX_SQUARE_NONLINEAR_SINCOS\")\n",
    "    gmsh.model.addPhysicalGroup(1, [1,2,4], name=\"PRESSURE_SQUARE_SINCOS\")\n",
    "    gmsh.model.addPhysicalGroup(2, [square1], name=\"SOURCE_SQUARE_NONLINEAR_SINCOS\")\n",
    "\n",
    "    # for possible square meshes\n",
    "    if not params.triangle_mesh:\n",
    "        gmsh.option.setNumber(\"Mesh.RecombineAll\", 1)\n",
    "\n",
    "    # generate a 3D mesh\n",
    "    gmsh.model.mesh.generate(2)\n",
    "    \n",
    "    # save as a .med file\n",
    "    params.med_file = params.mesh_file + \".med\"\n",
    "    gmsh.write(params.med_file)\n",
    "    \n",
    "    # close gmsh\n",
    "    gmsh.finalize()\n",
    "    \n",
    "    readMed(params)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mexi hat analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of a config file - what attributes should the blocksets have\n",
    "def generateConfig_squareMexiHat(params):\n",
    "    # Open the file for writing\n",
    "    with open(params.config_file, 'w') as f:\n",
    "        # FLUX_SQUARE_SINCOS boundary condition\n",
    "        data = ['[SET_ATTR_FLUX_UNIFORM]', 'number_of_attributes=1', 'user1=0.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # PRESSURE_SQUARE_SINCOS boundary condition\n",
    "        data = ['[SET_ATTR_PRESSURE_UNIFORM]', 'number_of_attributes=1', 'user1=0.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # SOURCE_SQUARE_SINCOS\n",
    "        data = ['[SET_ATTR_SOURCE_MEXI]', 'number_of_attributes=1', 'user1=1.0', ' ']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMesh_squareMexiHat(params):\n",
    "    \n",
    "    # Initialize gmsh\n",
    "    gmsh.initialize()\n",
    "    gmsh.option.setNumber(\"General.Verbosity\", 3)\n",
    "    \n",
    "    tol = 0.1\n",
    "    L = params.length_x\n",
    "    point1 = gmsh.model.geo.addPoint(-L/2, -L/2, 0, tol)\n",
    "    point2 = gmsh.model.geo.addPoint(L/2, -L/2, 0, tol)\n",
    "    point3 = gmsh.model.geo.addPoint(L/2, L/2, 0, tol)\n",
    "    point4 = gmsh.model.geo.addPoint(-L/2, L/2, 0, tol)\n",
    "\n",
    "    line1 = gmsh.model.geo.addLine(point1, point2)\n",
    "    line2 = gmsh.model.geo.addLine(point2, point3)\n",
    "    line3 = gmsh.model.geo.addLine(point3, point4)\n",
    "    line4 = gmsh.model.geo.addLine(point4, point1)\n",
    "\n",
    "    curve_loop = gmsh.model.geo.addCurveLoop([line1, line2, line3, line4])\n",
    "    plane_surface = gmsh.model.geo.addPlaneSurface([curve_loop])\n",
    "    \n",
    "    N = int(L / params.element_size) + 1\n",
    "    \n",
    "    gmsh.model.geo.mesh.setTransfiniteCurve(line1, N)\n",
    "    gmsh.model.geo.mesh.setTransfiniteCurve(line2, N)\n",
    "    gmsh.model.geo.mesh.setTransfiniteCurve(line3, N)\n",
    "    gmsh.model.geo.mesh.setTransfiniteCurve(line4, N)\n",
    "    gmsh.model.geo.mesh.setTransfiniteSurface(plane_surface)\n",
    "    \n",
    "    gmsh.model.geo.synchronize()\n",
    "\n",
    "    # gmsh.model.addPhysicalGroup(dimention, [number of element], name=\"name\")\n",
    "    gmsh.model.addPhysicalGroup(2, [plane_surface], name=\"SOURCE_MEXI\")\n",
    "    # gmsh.model.addPhysicalGroup(1, [1, 2, 3, 4], name=\"PRESSURE_UNIFORM\")\n",
    "    gmsh.model.addPhysicalGroup(1, [1, 2, 3], name=\"PRESSURE_UNIFORM\")\n",
    "    gmsh.model.addPhysicalGroup(1, [4], name=\"FLUX_UNIFORM\")\n",
    "\n",
    "    # for possible square meshes\n",
    "    if not params.triangle_mesh:\n",
    "        gmsh.option.setNumber(\"Mesh.RecombineAll\", 1)\n",
    "\n",
    "    # generate a 2D mesh\n",
    "    gmsh.model.mesh.generate(2)\n",
    "\n",
    "    # save as a .med file\n",
    "    params.med_file = params.mesh_file + \".med\"\n",
    "    gmsh.write(params.med_file)\n",
    "    \n",
    "    # close gmsh\n",
    "    gmsh.finalize()\n",
    "    \n",
    "    readMed(params)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L shape analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConfig_Lshape(params):\n",
    "    # Open the file for writing\n",
    "    with open(params.config_file, 'w') as f:\n",
    "        # FLUX_L boundary condition\n",
    "        data = ['[SET_ATTR_FLUX_L]', 'number_of_attributes=1', 'user1=1']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # PRESSURE_UNIFORM boundary condition\n",
    "        data = ['[SET_ATTR_PRESSURE_UNIFORM]', 'number_of_attributes=1', 'user1=0']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)\n",
    "        # PRESSURE_L boundary condition\n",
    "        data = ['[SET_ATTR_PRESSURE_L]', 'number_of_attributes=1', 'user1=1']\n",
    "        # Use a for loop to write each line of data to the file\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')\n",
    "            # print the data as it is written to the file\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMesh_Lshape(params):\n",
    "    \n",
    "    # Initialize gmsh\n",
    "    gmsh.initialize()\n",
    "    gmsh.option.setNumber(\"General.Verbosity\", 3)\n",
    "    \n",
    "    tol = 0.001\n",
    "    L = 1\n",
    "    \n",
    "    point1 = gmsh.model.geo.addPoint(0, 0, 0, tol)\n",
    "    point2 = gmsh.model.geo.addPoint(0, -1, 0, tol)\n",
    "    point3 = gmsh.model.geo.addPoint(1, -1, 0, tol)\n",
    "    point4 = gmsh.model.geo.addPoint(1, 1, 0, tol)\n",
    "    point5 = gmsh.model.geo.addPoint(-1, 1, 0, tol)\n",
    "    point6 = gmsh.model.geo.addPoint(-1, 0, 0, tol)\n",
    "    \n",
    "    line1 = gmsh.model.geo.addLine(point1, point2);\n",
    "    line2 = gmsh.model.geo.addLine(point2, point3);\n",
    "    line3 = gmsh.model.geo.addLine(point3, point4);\n",
    "    line4 = gmsh.model.geo.addLine(point4, point5);\n",
    "    line5 = gmsh.model.geo.addLine(point5, point6);\n",
    "    line6 = gmsh.model.geo.addLine(point6, point1);\n",
    "    \n",
    "    curve_loop_1 = gmsh.model.geo.addCurveLoop([line1, line2,line3,line4,line5,line6])\n",
    "    plane_surface_1 = gmsh.model.geo.addPlaneSurface([curve_loop_1])\n",
    "\n",
    "    # Create the relevant Gmsh data structures from Gmsh model.\n",
    "    gmsh.model.geo.synchronize()\n",
    "\n",
    "    gmsh.option.setNumber(\"Mesh.MeshSizeMin\", params.element_size)\n",
    "    gmsh.option.setNumber(\"Mesh.MeshSizeMax\", params.element_size)\n",
    "\n",
    "    # gmsh.model.addPhysicalGroup(dimention, [number of element], name=\"name\")\n",
    "    gmsh.model.addPhysicalGroup(2, [1], name=\"DOMAIN\")\n",
    "    gmsh.model.addPhysicalGroup(1, [line1,line6], name=\"PRESSURE_L\")\n",
    "    gmsh.model.addPhysicalGroup(1, [line2,line3,line4,line5], name=\"FLUX_L\")\n",
    "\n",
    "    # generate a 2D mesh\n",
    "    gmsh.model.mesh.generate(2)\n",
    "    \n",
    "    # save as a .med file\n",
    "    params.med_file = params.mesh_file + \".med\"\n",
    "    gmsh.write(params.med_file)\n",
    "    \n",
    "    # close gmsh\n",
    "    gmsh.finalize()\n",
    "    \n",
    "    readMed(params)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
